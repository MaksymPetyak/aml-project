{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Dense, MaxPool2D\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Softmax, Reshape, Flatten\n",
    "from tensorflow.keras.layers import concatenate, maximum, Lambda, Layer\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "import tensorflow.keras.backend as K\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional layers not available in core Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bias(Layer):\n",
    "    \"\"\"\n",
    "    Adds bias to a layer. This is used for untied biases convolution. \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Bias, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.bias = self.add_weight(name='bias', \n",
    "                                      shape=input_shape[1:],\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Bias, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.add(x, self.bias)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pool_max(input, pool_size=2, axis=1):\n",
    "    \"\"\"\n",
    "    Based on lasagne implementation of FeaturePool\n",
    "    \"\"\"\n",
    "    input_shape = input.shape.as_list()\n",
    "    num_feature_maps = input_shape[axis]\n",
    "    num_feature_maps_out = num_feature_maps // pool_size\n",
    "    \n",
    "    pool_shape = tf.TensorShape(\n",
    "        (input_shape[1:axis] + [num_feature_maps_out, pool_size] + input_shape[axis+1:])\n",
    "    )\n",
    "    \n",
    "    print(pool_shape)\n",
    "    input_reshaped = Reshape(pool_shape)(input)\n",
    "    # reduce along all axis but the target one\n",
    "    reduction_axis = list(range(1, len(pool_shape)+1))\n",
    "    reduction_axis.pop(axis-1)\n",
    "    \n",
    "    return tf.reduce_max(input_reshaped, axis=reduction_axis)\n",
    "\n",
    "# create Layer for reshape with batchsize\n",
    "# strong_reshape_func = lambda x: tf.reshape(x, (batch_size//2, concat.shape[1]*2))\n",
    "# StrongReshape = Lambda(strong_reshape_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(width=512, height=512, filename=None,\n",
    "                n_classes=5, batch_size=64, p_conv=0.0):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input shape (height, width, depth), different from original implimentation\n",
    "    main_input = Input(shape=(height, width, 3),\n",
    "                       batch_size=batch_size,\n",
    "                      )\n",
    "    \n",
    "    # Note: for conv layers paper uses untie_biases=True\n",
    "    # layer will have separate bias parameters for each position in each channel. \n",
    "    # As a result, the b attribute will be a 3D tensor.\n",
    "\n",
    "    # no need to init weights as they will be loaded from a file\n",
    "    # Conv layers(filters, kernel_size)\n",
    "    conv_main_1 = Conv2D(32, 7, strides=(2, 2), padding='same', \n",
    "                    use_bias=False,\n",
    "                    activation= None,\n",
    "                   )(main_input)\n",
    "    conv_bias_1 = Bias()(conv_main_1)\n",
    "    conv_activation_1 = LeakyReLU(alpha=0.5)(conv_bias_1)\n",
    "    dropout_1 = Dropout(p_conv)(conv_activation_1)\n",
    "    maxpool_1 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_1)\n",
    "    # 3\n",
    "    conv_main_2 = Conv2D(32, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation= None,\n",
    "                        )(maxpool_1)\n",
    "    conv_bias_2 = Bias()(conv_main_2)\n",
    "    conv_activation_2 = LeakyReLU(alpha=0.5)(conv_bias_2)\n",
    "    dropout_2 = Dropout(p_conv)(conv_activation_2)\n",
    "    # 4\n",
    "    conv_main_3 = Conv2D(32, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation= None,\n",
    "                        )(dropout_2)\n",
    "    conv_bias_3 = Bias()(conv_main_3)\n",
    "    conv_activation_3 = LeakyReLU(alpha=0.5)(conv_bias_3)\n",
    "    dropout_3 = Dropout(p_conv)(conv_activation_3)\n",
    "    maxpool_3 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_3)\n",
    "    # 6\n",
    "    conv_main_4 = Conv2D(64, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_3)\n",
    "    conv_bias_4 = Bias()(conv_main_4)\n",
    "    conv_activation_4 = LeakyReLU(alpha=0.5)(conv_bias_4)\n",
    "    dropout_4 = Dropout(p_conv)(conv_activation_4)\n",
    "    # 7\n",
    "    conv_main_5 = Conv2D(64, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_4)\n",
    "    conv_bias_5 = Bias()(conv_main_5)\n",
    "    conv_activation_5 = LeakyReLU(alpha=0.5)(conv_bias_5)\n",
    "    dropout_5 = Dropout(p_conv)(conv_activation_5)\n",
    "    maxpool_5 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_5)\n",
    "    # 9 \n",
    "    conv_main_6 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_5)\n",
    "    conv_bias_6 = Bias()(conv_main_6)\n",
    "    conv_activation_6 = LeakyReLU(alpha=0.5)(conv_bias_6)\n",
    "    dropout_6 = Dropout(p_conv)(conv_activation_6)\n",
    "    # 10    \n",
    "    conv_main_7 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_6)\n",
    "    conv_bias_7 = Bias()(conv_main_7)\n",
    "    conv_activation_7 = LeakyReLU(alpha=0.5)(conv_bias_7)\n",
    "    dropout_7 = Dropout(p_conv)(conv_activation_7)\n",
    "    # 11\n",
    "    conv_main_8 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_7)\n",
    "    conv_bias_8 = Bias()(conv_main_8)\n",
    "    conv_activation_8 = LeakyReLU(alpha=0.5)(conv_bias_8)\n",
    "    dropout_8 = Dropout(p_conv)(conv_activation_8)\n",
    "    # 12\n",
    "    conv_main_9 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_8)\n",
    "    conv_bias_9 = Bias()(conv_main_9)\n",
    "    conv_activation_9 = LeakyReLU(alpha=0.5)(conv_bias_9)\n",
    "    dropout_9 = Dropout(p_conv)(conv_activation_9)\n",
    "    maxpool_9 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_9)\n",
    "    # 14\n",
    "    conv_main_10 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_9)\n",
    "    conv_bias_10 = Bias()(conv_main_10)\n",
    "    conv_activation_10 = LeakyReLU(alpha=0.5)(conv_bias_10)\n",
    "    dropout_10 = Dropout(p_conv)(conv_activation_10)\n",
    "    # 15\n",
    "    conv_main_11 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_10)\n",
    "    conv_bias_11 = Bias()(conv_main_11)\n",
    "    conv_activation_11 = LeakyReLU(alpha=0.5)(conv_bias_11)\n",
    "    dropout_11 = Dropout(p_conv)(conv_activation_11)\n",
    "    # 16\n",
    "    conv_main_12 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_11)\n",
    "    conv_bias_12 = Bias()(conv_main_12)\n",
    "    conv_activation_12 = LeakyReLU(alpha=0.5)(conv_bias_12)\n",
    "    dropout_12 = Dropout(p_conv)(conv_activation_12)\n",
    "    # 17\n",
    "    conv_main_13 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_12)\n",
    "    conv_bias_13 = Bias()(conv_main_13)\n",
    "    conv_activation_13 = LeakyReLU(alpha=0.5)(conv_bias_13)\n",
    "    dropout_13 = Dropout(p_conv)(conv_activation_13)\n",
    "    maxpool_13 = MaxPool2D(pool_size=3, name=\"last_conv\", strides=(2, 2))(dropout_13)\n",
    "    # 19, special dropout between phases with p=1/2 \n",
    "    dropout_inter = Dropout(0.5)(maxpool_13)\n",
    "    # 20 Dense phase \n",
    "    # Begins with Maxout layer, which is implemented here as Dense+custom feature_pool function\n",
    "    flatten_inter = Flatten()(dropout_inter)\n",
    "    maxout_1 = Dense(units=1024, \n",
    "                     activation=None)(flatten_inter)\n",
    "    # need to wrap operation in Lambda to count as a layer\n",
    "    maxout_2 = Lambda(lambda x: feature_pool_max(x, pool_size=2, axis=1))(maxout_1)\n",
    "    \n",
    "    # 22 Concatenate with processed img, take both eyes into account\n",
    "    img_dim_input = Input(shape=(2,), batch_size=batch_size, name=\"imgdim\")\n",
    "    concat = concatenate([maxout_2, img_dim_input], axis=1)\n",
    "    \n",
    "    # 24\n",
    "    # flatten = Reshape((-1, concat.output_shape()[1] * 2))(concat)\n",
    "    print(concat.shape)\n",
    "    # use lambda for custom reshape that's capable of changing batch_size as well\n",
    "    # expect order left-right\n",
    "    # TODO: (-1, net['23'].output_shape[1] * 2)\n",
    "    flatten = Lambda(lambda x: K.reshape(x, (-1, concat.shape[1]*2)))(concat)\n",
    "    dense_droupout_0 = Dropout(0.5)(flatten)\n",
    "    # 26\n",
    "    dense_1 = Dense(units=1024,\n",
    "                    activation=None,\n",
    "                   )(dense_droupout_0)\n",
    "    dense_maxpool_1 = Lambda(lambda x: feature_pool_max(x, pool_size=2, axis=1))(dense_1)\n",
    "    dense_dropout_1 = Dropout(0.5)(dense_maxpool_1)\n",
    "    \n",
    "    # 29\n",
    "    dense_2 = Dense(units=n_classes*2,\n",
    "                    activation=None,\n",
    "                   )(dense_dropout_1)\n",
    "    softmax_flatten = Lambda(lambda x: tf.reshape(x, (-1, n_classes)))(dense_2)\n",
    "    softmax = keras.layers.Softmax()(softmax_flatten)\n",
    "    \n",
    "    model = Model(inputs=[main_input, img_dim_input], outputs=[softmax])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(512, 2)\n",
      "(64, 514)\n",
      "(512, 2)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (64, 512, 512, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (64, 256, 256, 32)   4704        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bias (Bias)                     (64, 256, 256, 32)   2097152     conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (64, 256, 256, 32)   0           bias[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (64, 256, 256, 32)   0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (64, 127, 127, 32)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (64, 127, 127, 32)   9216        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bias_1 (Bias)                   (64, 127, 127, 32)   516128      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (64, 127, 127, 32)   0           bias_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (64, 127, 127, 32)   0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (64, 127, 127, 32)   9216        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_2 (Bias)                   (64, 127, 127, 32)   516128      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (64, 127, 127, 32)   0           bias_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (64, 127, 127, 32)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (64, 63, 63, 32)     0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (64, 63, 63, 64)     18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bias_3 (Bias)                   (64, 63, 63, 64)     254016      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (64, 63, 63, 64)     0           bias_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (64, 63, 63, 64)     0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (64, 63, 63, 64)     36864       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_4 (Bias)                   (64, 63, 63, 64)     254016      conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (64, 63, 63, 64)     0           bias_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (64, 63, 63, 64)     0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (64, 31, 31, 64)     0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (64, 31, 31, 128)    73728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bias_5 (Bias)                   (64, 31, 31, 128)    123008      conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (64, 31, 31, 128)    0           bias_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (64, 31, 31, 128)    0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (64, 31, 31, 128)    147456      dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_6 (Bias)                   (64, 31, 31, 128)    123008      conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (64, 31, 31, 128)    0           bias_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (64, 31, 31, 128)    0           leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (64, 31, 31, 128)    147456      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_7 (Bias)                   (64, 31, 31, 128)    123008      conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (64, 31, 31, 128)    0           bias_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (64, 31, 31, 128)    0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (64, 31, 31, 128)    147456      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_8 (Bias)                   (64, 31, 31, 128)    123008      conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (64, 31, 31, 128)    0           bias_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (64, 31, 31, 128)    0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (64, 15, 15, 128)    0           dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (64, 15, 15, 256)    294912      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bias_9 (Bias)                   (64, 15, 15, 256)    57600       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (64, 15, 15, 256)    0           bias_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (64, 15, 15, 256)    0           leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (64, 15, 15, 256)    589824      dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bias_10 (Bias)                  (64, 15, 15, 256)    57600       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (64, 15, 15, 256)    0           bias_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (64, 15, 15, 256)    0           leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (64, 15, 15, 256)    589824      dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bias_11 (Bias)                  (64, 15, 15, 256)    57600       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (64, 15, 15, 256)    0           bias_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (64, 15, 15, 256)    0           leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (64, 15, 15, 256)    589824      dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bias_12 (Bias)                  (64, 15, 15, 256)    57600       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (64, 15, 15, 256)    0           bias_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (64, 15, 15, 256)    0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "last_conv (MaxPooling2D)        (64, 7, 7, 256)      0           dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (64, 7, 7, 256)      0           last_conv[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (64, 12544)          0           dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (64, 1024)           12846080    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (64, 512)            0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "imgdim (InputLayer)             (64, 2)              0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (64, 514)            0           lambda[0][0]                     \n",
      "                                                                 imgdim[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (32, 1028)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (32, 1028)           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (32, 1024)           1053696     dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (32, 512)            0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (32, 512)            0           lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (32, 10)             5130        dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (64, 5)              0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Softmax)               (64, 5)              0           lambda_3[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 20,923,690\n",
      "Trainable params: 20,923,690\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to unicode compatability issues between python2 and python3 we use np.save method, with encoding as 'bytes'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_names = ['Conv_1_W', 'Conv_1_b', 'Conv_2_W', 'Conv_2_b', 'Conv_3_W', 'Conv_3_b', \n",
    "                'Conv_4_W', 'Conv_4_b', 'Conv_5_W', 'Conv_5_b', 'Conv_6_W', 'Conv_6_b', \n",
    "                'Conv_7_W', 'Conv_7_b', 'Conv_8_W', 'Conv_8_b', 'Conv_9_W', 'Conv_9_b', \n",
    "                'Conv_10_W', 'Conv_10_b', 'Conv_11_W', 'Conv_11_b', 'Conv_12_W', 'Conv_12_b', \n",
    "                'Conv_13_W', 'Conv_13_b', 'Dense_1_W', 'Dense_1_b', 'Dense_2_W', 'Dense_2_b', \n",
    "                'Dense_3_W', 'Dense_3_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.load(\"../models/JFnet_weights.npy\", encoding=\"bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5916246 ,  0.04647866,  0.40189323, -0.12301593, -0.41697878,\n",
       "        0.6376505 ,  0.02411902,  0.393416  , -0.03656115, -0.51861304],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_to_weigts = dict(zip(params_names, arr))\n",
    "layer_to_weigts['Dense_3_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all layers in the model\n",
    "c_conv = 1\n",
    "c_dense = 1\n",
    "# lasagne store weights not their transpose, so we transpose\n",
    "for layer in model.layers:\n",
    "    layer_str = str(layer)\n",
    "    if \"Conv2D\" in layer_str:\n",
    "        layer.set_weights([\n",
    "            layer_to_weigts['Conv_{}_W'.format(c_conv)].T\n",
    "        ])\n",
    "    if \"Bias\" in layer_str:\n",
    "        layer.set_weights([\n",
    "            layer_to_weigts['Conv_{}_b'.format(c_conv)].T\n",
    "        ])\n",
    "        c_conv +=1\n",
    "    if \"Dense\" in layer_str:\n",
    "        layer.set_weights([\n",
    "            layer_to_weigts['Dense_{}_W'.format(c_dense)],\n",
    "            layer_to_weigts['Dense_{}_b'.format(c_dense)]\n",
    "        ])\n",
    "        c_dense += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models with these weights for future use in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"../models/keras_JFnet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 2)\n",
      "(64, 514)\n",
      "(512, 2)\n"
     ]
    }
   ],
   "source": [
    "model2 = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(64, 512, 512, 3), (64, 2)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905],\n",
       "       [0.29261371, 0.26478586, 0.3295034 , 0.0867935 , 0.02630354],\n",
       "       [0.30382672, 0.26224723, 0.3291842 , 0.08550265, 0.01923905]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([np.zeros((64, 512, 512, 3)), np.zeros((64, 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.load_weights(\"../models/keras_JFnet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Peeling off layers and BCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_output = model2.get_layer(\"last_conv\").output\n",
    "\n",
    "mean_pooled = GlobalAveragePooling2D(\n",
    "    data_format='channels_last')(conv_output)\n",
    "max_pooled = GlobalMaxPooling2D(\n",
    "    data_format='channels_last')(conv_output)\n",
    "global_pool = concatenate([mean_pooled, max_pooled], axis=1)\n",
    "\n",
    "softmax_input = Dense(\n",
    "    units=n_classes, activation=None,)(global_pool)\n",
    "softmax_output = Softmax()(softmax_input)\n",
    "\n",
    "model = Model(inputs=[model2.input[0]], outputs=[softmax_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = np.zeros(model.input_shape)\n",
    "model2_input = [np.zeros((64, 512, 512, 3)), np.zeros((64, 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction with uncertainty\n",
    "f = K.function(\n",
    "    [model2.input, K.learning_phase()],\n",
    "    model2.layers[-1].output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32151932, 0.27409336, 0.31912398, 0.06523036, 0.02003301],\n",
       "       [0.2946003 , 0.27053407, 0.3411618 , 0.07794563, 0.01575826],\n",
       "       [0.2227369 , 0.29328418, 0.39758313, 0.0780096 , 0.00838627],\n",
       "       [0.2041975 , 0.24208054, 0.440249  , 0.10649191, 0.00698102],\n",
       "       [0.28511122, 0.2669041 , 0.3204653 , 0.10092761, 0.02659171],\n",
       "       [0.32106453, 0.28829306, 0.2952235 , 0.08379081, 0.01162801],\n",
       "       [0.3828203 , 0.28605708, 0.25532675, 0.0646798 , 0.011116  ],\n",
       "       [0.4369431 , 0.25912952, 0.22344156, 0.06546942, 0.01501635],\n",
       "       [0.26065677, 0.30549487, 0.34569398, 0.06842128, 0.01973314],\n",
       "       [0.2874376 , 0.3040156 , 0.3263004 , 0.06041883, 0.02182759],\n",
       "       [0.35176748, 0.31733122, 0.2580672 , 0.05438588, 0.01844825],\n",
       "       [0.396638  , 0.29135346, 0.25005224, 0.05214821, 0.00980814],\n",
       "       [0.38699618, 0.28578606, 0.26683822, 0.04985572, 0.01052393],\n",
       "       [0.41497925, 0.26935127, 0.26443264, 0.04506559, 0.00617116],\n",
       "       [0.36954695, 0.26886895, 0.2672654 , 0.07433151, 0.01998711],\n",
       "       [0.31507277, 0.27603045, 0.3104989 , 0.082972  , 0.0154258 ],\n",
       "       [0.30492055, 0.25820366, 0.34939244, 0.06616835, 0.021315  ],\n",
       "       [0.26249593, 0.3022164 , 0.35258764, 0.06716815, 0.01553186],\n",
       "       [0.3364234 , 0.28718728, 0.30254942, 0.06107422, 0.01276559],\n",
       "       [0.27934378, 0.25534013, 0.37873256, 0.07276198, 0.01382155],\n",
       "       [0.2109806 , 0.22311069, 0.40462244, 0.13416705, 0.02711925],\n",
       "       [0.282401  , 0.25782236, 0.3473638 , 0.09643836, 0.01597443],\n",
       "       [0.25085407, 0.32375246, 0.34780505, 0.06814047, 0.0094479 ],\n",
       "       [0.2871263 , 0.29308426, 0.34118113, 0.07112939, 0.00747895],\n",
       "       [0.29302773, 0.2630758 , 0.33817884, 0.09188405, 0.01383357],\n",
       "       [0.26018175, 0.23873521, 0.390337  , 0.09797225, 0.01277384],\n",
       "       [0.26912427, 0.31463432, 0.33636135, 0.06066724, 0.0192128 ],\n",
       "       [0.24876717, 0.28124192, 0.39268583, 0.06783395, 0.00947115],\n",
       "       [0.2916941 , 0.2895927 , 0.31326896, 0.07405017, 0.03139403],\n",
       "       [0.29892612, 0.2788404 , 0.32588533, 0.08188397, 0.01446414],\n",
       "       [0.32872185, 0.24685253, 0.2799354 , 0.11576638, 0.02872396],\n",
       "       [0.3338401 , 0.21570466, 0.31565228, 0.11150966, 0.02329331],\n",
       "       [0.3116557 , 0.27023822, 0.3310735 , 0.07379474, 0.01323778],\n",
       "       [0.249756  , 0.2692752 , 0.37258065, 0.09258645, 0.01580174],\n",
       "       [0.38469523, 0.29893064, 0.25786418, 0.04718642, 0.0113236 ],\n",
       "       [0.40429974, 0.27211493, 0.26850092, 0.04681702, 0.00826736],\n",
       "       [0.27509314, 0.30547374, 0.31240067, 0.07541246, 0.03161994],\n",
       "       [0.28209063, 0.24130736, 0.3425669 , 0.11209594, 0.02193913],\n",
       "       [0.36183357, 0.24108085, 0.2795609 , 0.08331244, 0.0342122 ],\n",
       "       [0.3432682 , 0.25437632, 0.3170087 , 0.06937808, 0.0159687 ],\n",
       "       [0.2396552 , 0.27573285, 0.34353226, 0.11342301, 0.02765666],\n",
       "       [0.22052148, 0.2634632 , 0.37253922, 0.11686801, 0.0266081 ],\n",
       "       [0.26073998, 0.24725343, 0.34297785, 0.12180551, 0.02722325],\n",
       "       [0.3063582 , 0.29024422, 0.2866713 , 0.09106231, 0.02566399],\n",
       "       [0.2928182 , 0.29377773, 0.31577784, 0.07173031, 0.02589594],\n",
       "       [0.3539454 , 0.30061802, 0.24547207, 0.0795818 , 0.02038274],\n",
       "       [0.35295925, 0.2880567 , 0.28588387, 0.05733203, 0.01576809],\n",
       "       [0.33474717, 0.27286917, 0.31974912, 0.0586893 , 0.0139453 ],\n",
       "       [0.27645442, 0.28535378, 0.32903934, 0.08445134, 0.02470111],\n",
       "       [0.34849432, 0.3227593 , 0.2686269 , 0.05246807, 0.0076514 ],\n",
       "       [0.22213557, 0.23503917, 0.3928164 , 0.11848461, 0.03152421],\n",
       "       [0.23971643, 0.22127531, 0.37678236, 0.1344468 , 0.02777921],\n",
       "       [0.3574528 , 0.26803878, 0.2865778 , 0.07551903, 0.01241156],\n",
       "       [0.38803518, 0.30372107, 0.25612825, 0.04308946, 0.0090261 ],\n",
       "       [0.28409714, 0.26129013, 0.3306838 , 0.10781208, 0.01611684],\n",
       "       [0.31385532, 0.27545464, 0.29360753, 0.09698293, 0.02009958],\n",
       "       [0.2279937 , 0.24004644, 0.3797877 , 0.11262781, 0.03954437],\n",
       "       [0.2827197 , 0.25252098, 0.35631177, 0.08666785, 0.02177968],\n",
       "       [0.16105683, 0.19997026, 0.39613703, 0.16924393, 0.07359201],\n",
       "       [0.24782017, 0.22164388, 0.37495816, 0.12325624, 0.03232161],\n",
       "       [0.24761823, 0.2308872 , 0.39297706, 0.10316367, 0.02535394],\n",
       "       [0.2111262 , 0.20306048, 0.40550077, 0.14164904, 0.03866358],\n",
       "       [0.26959464, 0.24459442, 0.34308192, 0.10711773, 0.03561134],\n",
       "       [0.25745368, 0.24244744, 0.3757196 , 0.09849601, 0.02588339]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f([model2_input, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'softmax_2/Softmax:0' shape=(64, 2) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
