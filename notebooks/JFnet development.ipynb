{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Conv2D, Dropout, Dense, MaxPool2D\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Softmax, Reshape \n",
    "from tensorflow.keras.layers import concatenate, maximum, Lambda, Layer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional layers not available in core Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bias(Layer):\n",
    "    \"\"\"\n",
    "    Adds bias to a layer. This is used for untied biases convolution. \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Bias, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.bias = self.add_weight(name='bias', \n",
    "                                      shape=input_shape[1:],\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(Bias, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.add(x, self.bias)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pool_max(input, pool_size=2, axis=1):\n",
    "    \"\"\"\n",
    "    Based on lasagne implementation of FeaturePool\n",
    "    \"\"\"\n",
    "    input_shape = input.shape.as_list()\n",
    "    num_feature_maps = input_shape[axis]\n",
    "    num_feature_maps_out = num_feature_maps // pool_size\n",
    "    \n",
    "    pool_shape = tf.TensorShape(\n",
    "        (input_shape[1:axis] + [num_feature_maps_out, pool_size] + input_shape[axis+1:])\n",
    "    )\n",
    "    \n",
    "    print(pool_shape)\n",
    "    input_reshaped = Reshape(pool_shape)(input)\n",
    "    # reduce along all axis but the target one\n",
    "    reduction_axis = list(range(1, len(pool_shape)+1))\n",
    "    reduction_axis.pop(axis-1)\n",
    "    \n",
    "    return tf.reduce_max(input_reshaped, axis=reduction_axis)\n",
    "\n",
    "# create Layer for reshape with batchsize\n",
    "# strong_reshape_func = lambda x: tf.reshape(x, (batch_size//2, concat.shape[1]*2))\n",
    "# StrongReshape = Lambda(strong_reshape_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(width=512, height=512, filename=None,\n",
    "                n_classes=5, batch_size=64, p_conv=0.0):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Input shape (height, width, depth), different from original implimentation\n",
    "    main_input = Input(shape=(height, width, 3),\n",
    "                       batch_size=batch_size,\n",
    "                      )\n",
    "    \n",
    "    # Note: for conv layers paper uses untie_biases=True\n",
    "    # layer will have separate bias parameters for each position in each channel. \n",
    "    # As a result, the b attribute will be a 3D tensor.\n",
    "\n",
    "    # TODO: set names of layers, check init\n",
    "    # no need to init weights as they will be loaded from a file\n",
    "    # Conv layers(filters, kernel_size)\n",
    "    conv_main_1 = Conv2D(32, 7, strides=(2, 2), padding='same', \n",
    "                    use_bias=False,\n",
    "                    activation= None,\n",
    "                   )(main_input)\n",
    "    conv_bias_1 = Bias()(conv_main_1)\n",
    "    conv_activation_1 = LeakyReLU(alpha=0.5)(conv_bias_1)\n",
    "    dropout_1 = Dropout(p_conv)(conv_activation_1)\n",
    "    maxpool_1 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_1)\n",
    "    # 3\n",
    "    conv_main_2 = Conv2D(32, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation= None,\n",
    "                        )(maxpool_1)\n",
    "    conv_bias_2 = Bias()(conv_main_2)\n",
    "    conv_activation_2 = LeakyReLU(alpha=0.5)(conv_bias_2)\n",
    "    dropout_2 = Dropout(p_conv)(conv_activation_2)\n",
    "    # 4\n",
    "    conv_main_3 = Conv2D(32, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation= None,\n",
    "                        )(dropout_2)\n",
    "    conv_bias_3 = Bias()(conv_main_3)\n",
    "    conv_activation_3 = LeakyReLU(alpha=0.5)(conv_bias_3)\n",
    "    dropout_3 = Dropout(p_conv)(conv_main_3)\n",
    "    maxpool_3 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_3)\n",
    "    # 6\n",
    "    conv_main_4 = Conv2D(64, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_3)\n",
    "    conv_bias_4 = Bias()(conv_main_4)\n",
    "    conv_activation_4 = LeakyReLU(alpha=0.5)(conv_bias_4)\n",
    "    dropout_4 = Dropout(p_conv)(conv_activation_4)\n",
    "    # 7\n",
    "    conv_main_5 = Conv2D(64, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_4)\n",
    "    conv_bias_5 = Bias()(conv_main_5)\n",
    "    conv_activation_5 = LeakyReLU(alpha=0.5)(conv_bias_5)\n",
    "    dropout_5 = Dropout(p_conv)(conv_activation_5)\n",
    "    maxpool_5 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_5)\n",
    "    # 9 \n",
    "    conv_main_6 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_5)\n",
    "    conv_bias_6 = Bias()(conv_main_6)\n",
    "    conv_activation_6 = LeakyReLU(alpha=0.5)(conv_bias_6)\n",
    "    dropout_6 = Dropout(p_conv)(conv_activation_6)\n",
    "    # 10    \n",
    "    conv_main_7 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_6)\n",
    "    conv_bias_7 = Bias()(conv_main_7)\n",
    "    conv_activation_7 = LeakyReLU(alpha=0.5)(conv_bias_7)\n",
    "    dropout_7 = Dropout(p_conv)(conv_activation_7)\n",
    "    # 11\n",
    "    conv_main_8 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_7)\n",
    "    conv_bias_8 = Bias()(conv_main_8)\n",
    "    conv_activation_8 = LeakyReLU(alpha=0.5)(conv_bias_8)\n",
    "    dropout_8 = Dropout(p_conv)(conv_8)\n",
    "    # 12\n",
    "    conv_main_9 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_8)\n",
    "    conv_bias_9 = Bias()(conv_main_9)\n",
    "    conv_activation_9 = LeakyReLU(alpha=0.5)(conv_bias_9)\n",
    "    dropout_9 = Dropout(p_conv)(conv_activation_9)\n",
    "    maxpool_9 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_9)\n",
    "    # 14\n",
    "    conv_main_10 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(maxpool_9)\n",
    "    conv_bias_10 = Bias()(conv_main_10)\n",
    "    conv_activation_10 = LeakyReLU(alpha=0.5)(conv_bias_10)\n",
    "    dropout_10 = Dropout(p_conv)(conv_activation_10)\n",
    "    # 15\n",
    "    conv_main_11 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_10)\n",
    "    conv_bias_11 = Bias()(conv_main_11)\n",
    "    conv_activation_11 = LeakyReLU(alpha=0.5)(conv_bias_11)\n",
    "    dropout_11 = Dropout(p_conv)(conv_activation_11)\n",
    "    # 16\n",
    "    conv_main_12 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_11)\n",
    "    conv_bias_12 = Bias()(conv_main_12)\n",
    "    conv_activation_12 = LeakyReLU(alpha=0.5)(conv_bias_12)\n",
    "    dropout_12 = Dropout(p_conv)(conv_activation_12)\n",
    "    # 17\n",
    "    conv_main_13 = Conv2D(256, 3, strides=(1, 1), padding='same', \n",
    "                         use_bias=False,\n",
    "                         activation=None,\n",
    "                        )(dropout_12)\n",
    "    conv_bias_13 = Bias()(conv_main_13)\n",
    "    conv_activation_13 = LeakyReLU(alpha=0.5)(conv_bias_13)\n",
    "    dropout_13 = Dropout(p_conv)(conv_activation_13)\n",
    "    maxpool_13 = MaxPool2D(pool_size=3, strides=(2, 2))(dropout_13)\n",
    "    # 19, special dropout between phases with p=1/2 \n",
    "    dropout_inter = Dropout(0.5)(maxpool_13)\n",
    "    # 20 Dense phase \n",
    "    # Begins with Maxout layer, which \n",
    "    maxout_1 = Dense(units=1024, \n",
    "                     activation=None,)(dropout_inter)\n",
    "    # need to wrap operation in Lambda to count as a layer\n",
    "    maxout_2 = Lambda(lambda x: feature_pool_max(x, pool_size=2, axis=3))(maxout_1)\n",
    "    \n",
    "    # 22 Concatenate with processed img, take both eyes into account\n",
    "    img_dim_input = Input(shape=(2,), batch_size=batch_size, name=\"imgdim\")\n",
    "    concat = concatenate([maxout_2, img_dim_input], axis=1)\n",
    "    \n",
    "    # 24\n",
    "    # flatten = Reshape((-1, concat.output_shape()[1] * 2))(concat)\n",
    "    print(concat.shape)\n",
    "    # use lambda for custom reshape that's capable of changing batch_size as well\n",
    "    # expect order left-right\n",
    "    # TODO: (-1, net['23'].output_shape[1] * 2)\n",
    "    flatten = Lambda(lambda x: tf.reshape(x, (batch_size//2, concat.shape[1]*2)))(concat)\n",
    "    dense_droupout_0 = Dropout(0.5)(flatten)\n",
    "    # 26\n",
    "    dense_1 = Dense(units=1024,\n",
    "                    activation=None,\n",
    "                   )(dense_droupout_0)\n",
    "    dense_maxpool_1 = Lambda(lambda x: feature_pool_max(x, pool_size=2, axis=1))(dense_1)\n",
    "    dense_dropout_1 = Dropout(0.5)(dense_maxpool_1)\n",
    "    \n",
    "    # 29\n",
    "    dense_2 = Dense(units=n_classes*2,\n",
    "                    activation=None,\n",
    "                   )(dense_dropout_1)\n",
    "    softmax_flatten = Lambda(lambda x: tf.reshape(x, (batch_size, n_classes)))(dense_2)\n",
    "    softmax = keras.layers.Softmax()(softmax_flatten)\n",
    "    \n",
    "    model = Model(inputs=[main_input, img_dim_input], outputs=[softmax])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ecce7de985bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-c49dd3fbae1c>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(width, height, filename, n_classes, batch_size, p_conv)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mconv_bias_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_main_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mconv_activation_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_bias_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mdropout_8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;31m# 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     conv_main_9 = Conv2D(128, 3, strides=(1, 1), padding='same', \n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_8' is not defined"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=concatenate([tf.zeros((1, 1)), tf.zeros((1, 1))])\n",
    "x.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tf.TensorShape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_model().summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
